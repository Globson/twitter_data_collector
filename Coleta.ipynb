{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coleta usando a API do twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy  # pip3 install tweepy\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import threading\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criaJsonSaida(json_input):\n",
    "    return {\n",
    "        \"id\": json_input[\"id\"],\n",
    "        \"data_criacao\": json_input[\"created_at\"],\n",
    "        \"tweet\": json_input[\"text\"].lower(),\n",
    "        \"nome_usuario\": json_input[\"user\"][\"name\"],\n",
    "        \"e_sobre_a_empresa\" : None\n",
    "    }\n",
    "\n",
    "def Collect(api, searchQuery, maxTweets, tweetsPerQry, arquivo):\n",
    "    tweetsColetados = {\n",
    "        \"tweets\": []\n",
    "    }\n",
    "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "    # else default to no lower limit, go as far back as API allows\n",
    "    sinceId = None\n",
    "\n",
    "    # If results only below a specific ID are, set max_id to that ID.\n",
    "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "    max_id = -1\n",
    "\n",
    "    tweetCount = 0\n",
    "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery,  # the search query string\n",
    "                                            lang='pt-BR',\n",
    "                                            count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery,\n",
    "                                            lang='pt-BR',\n",
    "                                            count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery,\n",
    "                                            lang='pt-BR',\n",
    "                                            count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery,\n",
    "                                            lang='pt-BR',\n",
    "                                            count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets: \n",
    "#                 print(\"No more tweets found\")\n",
    "#                 break\n",
    "                time.sleep(900) #15 min #tirar e descometar linhas acima\n",
    "            else:\n",
    "                for tweet in new_tweets:\n",
    "                    tweetJson = criaJsonSaida(tweet._json)\n",
    "                    tweetsColetados[\"tweets\"] += [tweetJson]\n",
    "                tweetCount += len(new_tweets)\n",
    "                max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "    with open(arquivo, 'w') as f:\n",
    "        f.write(json.dumps(tweetsColetados, indent=2))\n",
    "        f.close()\n",
    "    print (\"Downloaded {0} tweets, Saved to {1}\\n\".format(tweetCount, arquivo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "api_secret_key = ''\n",
    "\n",
    "auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
    "\n",
    "api = tweepy.API(auth,\n",
    "                 wait_on_rate_limit=True,\n",
    "                 # Whether or not to print a notification when Tweepy is waiting for rate limits to replenish\n",
    "                 wait_on_rate_limit_notify=True\n",
    "                 )\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "\n",
    "consulta = 'sadia'  # this is what we're searching for\n",
    "max_tweets = 100000  # Some arbitrary large number\n",
    "max_tweets_por_consulta = 15  # this is the max the API permits\n",
    "arquivo_com_tweetes_coletados = 'tweets.json'  # We'll store the tweets in a text file.\n",
    "\n",
    "Collect(api, consulta, 100, max_tweets_por_consulta, arquivo_com_tweetes_coletados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando csv com os tweets a serem rotulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(arquivo_com_twittes_coletados, 'r') as f:\n",
    "    dic_tweets = json.loads(f.read())\n",
    "    tweetsDF = pd.read_json(json.dumps(dic_tweets['tweets']))\n",
    "nome_arquivo_com_tweetes_rotulados = 'tweets.csv'\n",
    "tweetsDF.to_csv(nome_arquivo_com_tweetes_rotulados, index_label='id', quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo o csv com os tweets rotulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo_com_tweetes_rotulados = 'tweetsTestHistoy.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_rotuladosDF = pd.read_csv(nome_arquivo_com_tweetes_rotulados, index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo um prÃ©-processamento nos textos dos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificando os brigramas e trigramas encontrados nos twittes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "\n",
    "ngram = {}\n",
    "for indice, linha in tweets_rotuladosDF.iterrows() :\n",
    "    tokens = tknzr.tokenize(linha[tweets_rotuladosDF.tweet.name])\n",
    "    for count in range(len(tokens)):\n",
    "        if(tokens[count] == 'sadia'):\n",
    "            bigram = (tokens[count - 1], tokens[count])\n",
    "            trigram = (tokens[count - 2], tokens[count - 1], tokens[count])\n",
    "            \n",
    "            if ngram.get(bigram) == None:\n",
    "                ngram[bigram] = {\"frequencia\" : 1, \"id_tweets\" : [indice]}\n",
    "            else:\n",
    "                valor = ngram.get(bigram)\n",
    "                valor[\"frequencia\"] += 1\n",
    "                valor[\"id_tweets\"] += [indice]\n",
    "            if ngram.get(trigram) == None:\n",
    "                ngram[trigram] = {\"frequencia\" : 1, \"id_tweets\" : [indice]}\n",
    "            else:\n",
    "                valor = ngram.get(trigram)\n",
    "                valor[\"frequencia\"] += 1\n",
    "                valor[\"id_tweets\"] += [indice]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('&', 'sadia'): {'frequencia': 1, 'id_tweets': [10]},\n",
       " ('aberta', 'e', 'sadia'): {'frequencia': 1, 'id_tweets': [5]},\n",
       " ('brincadeirinha', 'sadia'): {'frequencia': 1, 'id_tweets': [0]},\n",
       " ('conversa', 'sadia'): {'frequencia': 1, 'id_tweets': [4]},\n",
       " ('e', 'sadia'): {'frequencia': 1, 'id_tweets': [5]},\n",
       " ('jovem', '&', 'sadia'): {'frequencia': 1, 'id_tweets': [10]},\n",
       " ('sociedade', 'sadia'): {'frequencia': 2, 'id_tweets': [1, 7]},\n",
       " ('uma', 'brincadeirinha', 'sadia'): {'frequencia': 1, 'id_tweets': [0]},\n",
       " ('uma', 'conversa', 'sadia'): {'frequencia': 1, 'id_tweets': [4]},\n",
       " ('uma', 'sociedade', 'sadia'): {'frequencia': 2, 'id_tweets': [1, 7]}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
